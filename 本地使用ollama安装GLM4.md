# LbNote
æˆ‘æ¥ä¸ºæ‚¨æä¾›è¯¦ç»†çš„ **Ollama éƒ¨ç½² GLM4 å¹¶å¼€å¯ API è°ƒç”¨çš„å®Œæ•´æŒ‡å—**ï¼Œé’ˆå¯¹ 16GB å†…å­˜çš„é…ç½®è¿›è¡Œä¼˜åŒ–ã€‚

---

## **ä¸€ã€ç³»ç»Ÿå‡†å¤‡ä¸ Ollama å®‰è£…**

### **1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥**
```bash
# æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
uname -a
free -h  # ç¡®è®¤å†…å­˜16GB
df -h    # ç¡®è®¤ç£ç›˜ç©ºé—´

# æ£€æŸ¥GPUï¼ˆå¯é€‰ï¼‰
lspci | grep -i nvidia
nvidia-smi  # å¦‚æœæœ‰NVIDIA GPU
```

### **2. å®‰è£… Ollamaï¼ˆæ¨èæ–¹æ³•ï¼‰**
```bash
# æ–¹æ³•1ï¼šä¸€é”®å®‰è£…è„šæœ¬ï¼ˆLinux/macOSï¼‰
curl -fsSL https://ollama.com/install.sh | sh

# æ–¹æ³•2ï¼šæ‰‹åŠ¨å®‰è£…ï¼ˆLinuxï¼‰
# ä¸‹è½½å®‰è£…åŒ…
curl -L https://ollama.com/download/ollama-linux-amd64 -o /tmp/ollama
chmod +x /tmp/ollama
sudo mv /tmp/ollama /usr/local/bin/

# æ–¹æ³•3ï¼šWindows
# è®¿é—® https://ollama.com/download ä¸‹è½½.exeå®‰è£…åŒ…
```

### **3. éªŒè¯å®‰è£…**
```bash
# æ£€æŸ¥ç‰ˆæœ¬
ollama --version
# åº”è¯¥è¾“å‡ºç±»ä¼¼ï¼šollama version 0.1.29

# æ£€æŸ¥æœåŠ¡çŠ¶æ€ï¼ˆLinuxï¼‰
sudo systemctl status ollama  # æˆ– ollama serve
```

---

## **äºŒã€éƒ¨ç½² GLM4 æ¨¡å‹ï¼ˆé’ˆå¯¹ 16GB å†…å­˜ä¼˜åŒ–ï¼‰**

### **1. æ¨¡å‹é€‰æ‹©å»ºè®®**
å¯¹äº 16GB å†…å­˜ï¼Œ**å¿…é¡»ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬**ï¼š

| æ¨¡å‹ç‰ˆæœ¬ | æ‰€éœ€å†…å­˜ | å»ºè®® | é€‚ç”¨åœºæ™¯ |
|---------|---------|------|---------|
| **glm4:9b-chat-q4_K_M** | 6-8 GB | **æ¨è** | å¹³è¡¡æ€§èƒ½ä¸å†…å­˜ |
| **glm4:9b-chat-q4_0** | 5-7 GB | å¤‡é€‰ | æ›´çœå†…å­˜ |
| **glm4:9b-chat-q8_0** | 9-11 GB | å†…å­˜å……è¶³æ—¶é€‰ | è´¨é‡æ›´å¥½ |
| **glm4:9b-chat** | 16+ GB | ä¸æ¨è | å¯èƒ½å†…å­˜ä¸è¶³ |

### **2. ä¸‹è½½æ¨¡å‹ï¼ˆé€‰æ‹©ä¸€ç§ï¼‰**
```bash
# æ¨èï¼š4ä½é‡åŒ–ä¸­ç­‰è´¨é‡ï¼ˆé€Ÿåº¦/è´¨é‡å¹³è¡¡ï¼‰
ollama pull glm4:9b-chat-q4_K_M

# å¤‡é€‰ï¼šæ›´å°å†…å­˜å ç”¨
ollama pull glm4:9b-chat-q4_0

# å¦‚æœåç»­å‡çº§å†…å­˜ï¼Œå¯ä»¥ä¸‹è½½8ä½é‡åŒ–
# ollama pull glm4:9b-chat-q8_0

# æŸ¥çœ‹ä¸‹è½½è¿›åº¦å’Œå·²ä¸‹è½½æ¨¡å‹
ollama list
```

### **3. éªŒè¯æ¨¡å‹**
```bash
# ç®€å•æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
ollama run glm4:9b-chat-q4_K_M "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡å›å¤"
# æŒ‰ Ctrl+D é€€å‡º
```

---

## **ä¸‰ã€å¯åŠ¨ API æœåŠ¡**

### **1. åŸºç¡€å¯åŠ¨æ–¹å¼**
```bash
# æ–¹æ³•1ï¼šå‰å°è¿è¡Œï¼ˆæµ‹è¯•ç”¨ï¼‰
ollama serve
# ä¿æŒç»ˆç«¯æ‰“å¼€ï¼ŒAPIè¿è¡Œåœ¨ http://127.0.0.1:11434

# æ–¹æ³•2ï¼šåå°è¿è¡Œï¼ˆæ¨èï¼‰
ollama serve &
# æˆ–
nohup ollama serve > ~/ollama.log 2>&1 &

# æ–¹æ³•3ï¼šä½œä¸ºç³»ç»ŸæœåŠ¡ï¼ˆLinuxï¼‰
sudo systemctl enable ollama
sudo systemctl start ollama
```

### **2. éªŒè¯ API æœåŠ¡**
```bash
# æ£€æŸ¥APIæ˜¯å¦å“åº”
curl http://localhost:11434/api/tags

# åº”è¯¥è¿”å›ç±»ä¼¼ï¼š
# {"models":[{"name":"glm4:9b-chat-q4_K_M","modified_at":"2024-01-01T00:00:00Z"}]}
```

---

## **å››ã€API è°ƒç”¨æ–¹æ³•**

### **1. REST API è°ƒç”¨ç¤ºä¾‹**

#### **åŸºç¡€å¯¹è¯ API**ï¼š
```bash
# éæµå¼å“åº”
curl http://localhost:11434/api/chat -d '{
  "model": "glm4:9b-chat-q4_K_M",
  "messages": [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
  ],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "top_p": 0.9,
    "num_predict": 512
  }
}'
```

#### **æµå¼å“åº” API**ï¼š
```bash
# æµå¼å“åº”ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰
curl -N http://localhost:11434/api/chat -d '{
  "model": "glm4:9b-chat-q4_K_M",
  "messages": [
    {"role": "user", "content": "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•"}
  ],
  "stream": true
}' | while read -r line; do
    if [[ $line == data:* ]]; then
        content="${line#data: }"
        if [[ $content != "[DONE]" ]]; then
            echo -n "$content" | jq -r '.message.content // empty' 2>/dev/null || true
        fi
    fi
done
```

### **2. OpenAI å…¼å®¹ API**
Ollama åŸç”Ÿæ”¯æŒ OpenAI æ ¼å¼ï¼Œç«¯å£ `11434/v1`ï¼š

```bash
# åˆ—å‡ºå¯ç”¨æ¨¡å‹
curl http://localhost:11434/v1/models

# ä½¿ç”¨ OpenAI æ ¼å¼è°ƒç”¨
curl http://localhost:11434/v1/chat/completions -d '{
  "model": "glm4:9b-chat-q4_K_M",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
  ],
  "max_tokens": 500
}'
```

---

## **äº”ã€Python å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹**

### **1. å®‰è£… Python ä¾èµ–**
```bash
pip install requests openai
# æˆ–
pip install ollama  # å®˜æ–¹Pythonåº“
```

### **2. ä½¿ç”¨å®˜æ–¹ ollama åº“**
```python
import ollama

# ç®€å•è°ƒç”¨
response = ollama.chat(
    model='glm4:9b-chat-q4_K_M',
    messages=[
        {'role': 'user', 'content': 'ä½ å¥½'}
    ]
)
print(response['message']['content'])

# æµå¼å“åº”
stream = ollama.chat(
    model='glm4:9b-chat-q4_K_M',
    messages=[{'role': 'user', 'content': 'è®²ä¸ªç¬‘è¯'}],
    stream=True
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

### **3. ä½¿ç”¨ OpenAI å…¼å®¹å®¢æˆ·ç«¯**
```python
from openai import OpenAI

# é…ç½®å®¢æˆ·ç«¯
client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama'  # ä»»æ„éç©ºå­—ç¬¦ä¸²
)

# å¯¹è¯
response = client.chat.completions.create(
    model="glm4:9b-chat-q4_K_M",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
        {"role": "user", "content": "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ"}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)

# æµå¼å“åº”
stream = client.chat.completions.create(
    model="glm4:9b-chat-q4_K_M",
    messages=[{"role": "user", "content": "è§£é‡Šä¸€ä¸‹ç¥ç»ç½‘ç»œ"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='')
```

### **4. ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨**
```python
import requests
import json

def chat_with_glm(prompt, model="glm4:9b-chat-q4_K_M"):
    url = "http://localhost:11434/api/chat"
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9,
            "num_predict": 512
        }
    }
    
    try:
        response = requests.post(url, json=payload, timeout=60)
        response.raise_for_status()
        return response.json()["message"]["content"]
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

# ä½¿ç”¨ç¤ºä¾‹
answer = chat_with_glm("ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ")
print(answer)
```

---

## **å…­ã€æ€§èƒ½ä¼˜åŒ–é…ç½®**

### **1. åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶ï¼ˆä¼˜åŒ–16GBå†…å­˜ï¼‰**
åˆ›å»º `Modelfile`ï¼š
```dockerfile
FROM glm4:9b-chat-q4_K_M

# å†…å­˜ä¼˜åŒ–å‚æ•°
PARAMETER num_ctx 4096      # ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå‡å°‘å¯èŠ‚çœå†…å­˜
PARAMETER num_batch 512     # æ‰¹å¤„ç†å¤§å°
PARAMETER num_thread 4      # CPUçº¿ç¨‹æ•°

# ç”Ÿæˆå‚æ•°
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
```

åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹ï¼š
```bash
ollama create glm4-custom -f ./Modelfile
ollama run glm4-custom
```

### **2. å¯åŠ¨å‚æ•°ä¼˜åŒ–**
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–æ€§èƒ½
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_KEEP_ALIVE=5m

# å¸¦ä¼˜åŒ–å‚æ•°å¯åŠ¨
ollama serve --host 0.0.0.0 --port 11434
```

---

## **ä¸ƒã€ç›‘æ§ä¸ç®¡ç†**

### **1. æŸ¥çœ‹è¿è¡ŒçŠ¶æ€**
```bash
# æŸ¥çœ‹æ¨¡å‹è¿è¡ŒçŠ¶æ€
ollama ps

# æŸ¥çœ‹ç³»ç»Ÿèµ„æºå ç”¨
htop  # æˆ– top
# å¦‚æœä½¿ç”¨GPU
nvidia-smi
```

### **2. æ—¥å¿—æŸ¥çœ‹**
```bash
# æŸ¥çœ‹Ollamaæ—¥å¿—
journalctl -u ollama -f  # systemdæœåŠ¡
# æˆ–
tail -f ~/.ollama/logs/server.log
```

### **3. API å¥åº·æ£€æŸ¥è„šæœ¬**
åˆ›å»º `health_check.py`ï¼š
```python
import requests
import time

def check_ollama_health():
    endpoints = [
        ("http://localhost:11434/api/tags", "æ¨¡å‹åˆ—è¡¨"),
        ("http://localhost:11434/api/version", "ç‰ˆæœ¬ä¿¡æ¯"),
        ("http://localhost:11434/v1/models", "OpenAIç«¯ç‚¹")
    ]
    
    for url, desc in endpoints:
        try:
            start = time.time()
            resp = requests.get(url, timeout=5)
            latency = (time.time() - start) * 1000
            status = "âœ…" if resp.status_code == 200 else "âŒ"
            print(f"{status} {desc}: {resp.status_code} ({latency:.1f}ms)")
        except Exception as e:
            print(f"âŒ {desc}: è¿æ¥å¤±è´¥ - {str(e)}")

if __name__ == "__main__":
    check_ollama_health()
```

è¿è¡Œï¼š`python health_check.py`

---

## **å…«ã€Web UI ç•Œé¢ï¼ˆå¯é€‰ï¼‰**

### **1. ä½¿ç”¨ Open WebUI**
```bash
# ä½¿ç”¨Dockerè¿è¡Œï¼ˆéœ€è¦å®‰è£…Dockerï¼‰
docker run -d \
  --name open-webui \
  -p 3000:8080 \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --add-host=host.docker.internal:host-gateway \
  ghcr.io/open-webui/open-webui:main
```
è®¿é—®ï¼šhttp://localhost:3000

### **2. ä½¿ç”¨ç®€å•çš„èŠå¤©ç•Œé¢**
åˆ›å»º `web_chat.html`ï¼š
```html
<!DOCTYPE html>
<html>
<head>
    <title>GLM4 èŠå¤©ç•Œé¢</title>
    <style>
        body { font-family: Arial; max-width: 800px; margin: auto; }
        #chat { border: 1px solid #ccc; height: 400px; overflow-y: auto; padding: 10px; }
        .message { margin: 10px 0; }
        .user { text-align: right; color: blue; }
        .assistant { text-align: left; color: green; }
    </style>
</head>
<body>
    <h2>GLM4 èŠå¤©</h2>
    <div id="chat"></div>
    <input type="text" id="input" style="width: 70%;">
    <button onclick="sendMessage()">å‘é€</button>
    
    <script>
        async function sendMessage() {
            const input = document.getElementById('input');
            const message = input.value;
            if (!message) return;
            
            // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            addMessage('user', message);
            input.value = '';
            
            // è°ƒç”¨API
            const response = await fetch('http://localhost:11434/api/chat', {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({
                    model: 'glm4:9b-chat-q4_K_M',
                    messages: [{role: 'user', content: message}],
                    stream: false
                })
            });
            
            const data = await response.json();
            addMessage('assistant', data.message.content);
        }
        
        function addMessage(role, content) {
            const chat = document.getElementById('chat');
            const div = document.createElement('div');
            div.className = `message ${role}`;
            div.innerHTML = `<strong>${role}:</strong> ${content}`;
            chat.appendChild(div);
            chat.scrollTop = chat.scrollHeight;
        }
        
        // å›è½¦å‘é€
        document.getElementById('input').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') sendMessage();
        });
    </script>
</body>
</html>
```

---

## **ä¹ã€ç”Ÿäº§ç¯å¢ƒå»ºè®®**

### **1. ä½¿ç”¨ systemd æœåŠ¡ï¼ˆLinuxï¼‰**
åˆ›å»º `/etc/systemd/system/ollama.service`ï¼š
```ini
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
Type=simple
User=your_username
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_KEEP_ALIVE=24h"
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
```

å¯ç”¨æœåŠ¡ï¼š
```bash
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

### **2. é…ç½® Nginx åå‘ä»£ç†ï¼ˆå¯é€‰ï¼‰**
```nginx
# /etc/nginx/sites-available/ollama
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:11434;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # å¢åŠ è¶…æ—¶æ—¶é—´
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
}
```

---

## **åã€æ•…éšœæ’é™¤**

### **å¸¸è§é—®é¢˜è§£å†³**

#### **1. å†…å­˜ä¸è¶³é”™è¯¯**
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h

# è§£å†³æ–¹æ¡ˆï¼š
# 1. ä½¿ç”¨æ›´å°çš„é‡åŒ–æ¨¡å‹
ollama pull glm4:9b-chat-q4_0

# 2. å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
# åœ¨APIè°ƒç”¨ä¸­è®¾ç½®ï¼š "options": {"num_ctx": 2048}

# 3. å¢åŠ äº¤æ¢ç©ºé—´ï¼ˆä¸´æ—¶ï¼‰
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

#### **2. ç«¯å£è¢«å ç”¨**
```bash
# æŸ¥æ‰¾å ç”¨11434ç«¯å£çš„è¿›ç¨‹
sudo lsof -i :11434
# æˆ–
sudo netstat -tulpn | grep 11434

# åœæ­¢å†²çªè¿›ç¨‹æˆ–æ›´æ”¹Ollamaç«¯å£
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

#### **3. æ¨¡å‹ä¸‹è½½å¤±è´¥**
```bash
# è®¾ç½®é•œåƒæº
export OLLAMA_HOST=https://mirror.ollama.ai
ollama pull glm4:9b-chat-q4_K_M

# æˆ–æ‰‹åŠ¨ä¸‹è½½åå¯¼å…¥
# 1. ä» https://ollama.com/library/glm4 ä¸‹è½½æ¨¡å‹æ–‡ä»¶
# 2. ollama create glm4 -f ./Modelfile
# 3. ollama run glm4
```

#### **4. API å“åº”æ…¢**
```bash
# 1. æ£€æŸ¥CPUä½¿ç”¨ç‡
top

# 2. ä½¿ç”¨æ€§èƒ½æ›´å¥½çš„é‡åŒ–ç‰ˆæœ¬
ollama pull glm4:9b-chat-q4_K_M  # æ¯” q4_0 å¿«

# 3. è°ƒæ•´ç”Ÿæˆå‚æ•°
# å‡å°‘ num_predictï¼Œå¢åŠ  temperature
```

---

## **åä¸€ã€ä¸€é”®éƒ¨ç½²è„šæœ¬**

åˆ›å»º `deploy_glm4.sh`ï¼š
```bash
#!/bin/bash
# GLM4ä¸€é”®éƒ¨ç½²è„šæœ¬

echo "æ­£åœ¨éƒ¨ç½²GLM4..."

# 1. å®‰è£…Ollama
if ! command -v ollama &> /dev/null; then
    echo "å®‰è£…Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# 2. ä¸‹è½½æ¨¡å‹
echo "ä¸‹è½½GLM4æ¨¡å‹..."
ollama pull glm4:9b-chat-q4_K_M

# 3. å¯åŠ¨æœåŠ¡
echo "å¯åŠ¨OllamaæœåŠ¡..."
sudo systemctl stop ollama 2>/dev/null || true
sudo systemctl start ollama

# 4. ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 5

# 5. éªŒè¯
echo "éªŒè¯éƒ¨ç½²..."
curl -s http://localhost:11434/api/tags | grep -q "glm4" && echo "âœ… éƒ¨ç½²æˆåŠŸï¼" || echo "âŒ éƒ¨ç½²å¤±è´¥"

# 6. æ˜¾ç¤ºä½¿ç”¨ä¿¡æ¯
echo ""
echo "==============================="
echo "GLM4 éƒ¨ç½²å®Œæˆï¼"
echo "APIåœ°å€: http://localhost:11434"
echo "OpenAIå…¼å®¹ç«¯ç‚¹: http://localhost:11434/v1"
echo ""
echo "æµ‹è¯•å‘½ä»¤:"
echo "  curl http://localhost:11434/api/chat -d '{\"model\":\"glm4:9b-chat-q4_K_M\",\"messages\":[{\"role\":\"user\",\"content\":\"ä½ å¥½\"}]}'"
echo "==============================="
```

è¿è¡Œï¼š
```bash
chmod +x deploy_glm4.sh
./deploy_glm4.sh
```

---

## **æ€»ç»“éƒ¨ç½²æµç¨‹**
1. **å®‰è£… Ollama**ï¼šä¸€é”®è„šæœ¬å®‰è£…
2. **æ‹‰å–æ¨¡å‹**ï¼š`ollama pull glm4:9b-chat-q4_K_M`ï¼ˆé€‚åˆ16GBå†…å­˜ï¼‰
3. **å¯åŠ¨æœåŠ¡**ï¼š`ollama serve &` æˆ–é…ç½®ä¸ºç³»ç»ŸæœåŠ¡
4. **éªŒè¯ API**ï¼š`curl http://localhost:11434/api/tags`
5. **å¼€å§‹è°ƒç”¨**ï¼šä½¿ç”¨ REST API æˆ– OpenAI å…¼å®¹æ ¼å¼

**å…³é”®ç‚¹**ï¼šç”±äºæ‚¨åªæœ‰16GBå†…å­˜ï¼Œ**å¿…é¡»ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬**ï¼ˆq4_K_Mæˆ–q4_0ï¼‰ï¼Œå¦åˆ™ä¼šå†…å­˜ä¸è¶³ã€‚

ç°åœ¨æ‚¨å·²ç»æ‹¥æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„æœ¬åœ° GLM4 API æœåŠ¡ï¼å¯ä»¥å¼€å§‹é›†æˆåˆ°æ‚¨çš„åº”ç”¨ä¸­ã€‚éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æˆ–æœ‰å…¶ä»–é—®é¢˜è¯·éšæ—¶é—®æˆ‘ï¼ ğŸš€
